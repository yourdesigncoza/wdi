---
phase: 03-core-will-conversation
plan: 02
type: tdd
wave: 1
depends_on: []
files_modified:
  - backend/app/services/openai_service.py
  - backend/app/prompts/__init__.py
  - backend/app/prompts/system.py
  - backend/app/prompts/extraction.py
  - backend/tests/test_openai_service.py
  - backend/requirements.txt
autonomous: true
user_setup:
  - service: openai
    why: "AI conversation engine for will creation"
    env_vars:
      - name: OPENAI_API_KEY
        source: "OpenAI Dashboard -> API keys -> Create new secret key"

must_haves:
  truths:
    - "OpenAI service builds section-specific system prompts with current will state context"
    - "System prompts enforce friendly tone, UPL boundaries, and SA will-specific guidance per section"
    - "Extraction schemas parse natural language into structured will data (beneficiaries, assets, guardians, executor)"
    - "Streaming method yields text chunks from gpt-4o-mini"
  artifacts:
    - path: "backend/app/services/openai_service.py"
      provides: "OpenAIService class with stream_response and extract_will_data methods"
      contains: "class OpenAIService"
    - path: "backend/app/prompts/system.py"
      provides: "Section-specific system prompt builder"
      contains: "def build_system_prompt"
    - path: "backend/app/prompts/extraction.py"
      provides: "Pydantic models for OpenAI Structured Output extraction"
      contains: "class ExtractedWillData"
    - path: "backend/tests/test_openai_service.py"
      provides: "Tests for prompt building and extraction schema validation"
  key_links:
    - from: "backend/app/services/openai_service.py"
      to: "backend/app/prompts/system.py"
      via: "import build_system_prompt"
    - from: "backend/app/services/openai_service.py"
      to: "backend/app/prompts/extraction.py"
      via: "import ExtractedWillData for structured output parsing"
---

<objective>
Build the OpenAI service with section-aware system prompts, streaming response method, and structured data extraction for will creation conversations.

Purpose: This is the AI brain of the conversation engine -- it generates contextually aware responses per will section, enforces the friendly-but-legal-boundary tone, and extracts structured will data from natural language.
Output: OpenAIService class, system prompt templates, extraction Pydantic models, tests for prompt building.
</objective>

<execution_context>
@/home/laudes/.claude/get-shit-done/workflows/execute-plan.md
@/home/laudes/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/STATE.md
@.planning/phases/03-core-will-conversation/03-RESEARCH.md
@backend/app/services/upl_filter.py
@backend/app/config.py
</context>

<feature>
  <name>OpenAI Service with SA Will Prompting</name>
  <files>
    backend/app/services/openai_service.py
    backend/app/prompts/__init__.py
    backend/app/prompts/system.py
    backend/app/prompts/extraction.py
    backend/tests/test_openai_service.py
    backend/requirements.txt
  </files>
  <behavior>
    System prompt building:
    - build_system_prompt("beneficiaries", {testator: {firstName: "John"}}) -> includes base personality + beneficiary-specific guidance + will state summary
    - build_system_prompt("guardians", {}) -> includes empathetic tone guidance for sensitive topic
    - All prompts include UPL boundary instruction ("NEVER give legal advice")
    - All prompts include collected data summary so AI doesn't re-ask

    Will state formatting:
    - format_will_summary({testator: {firstName: "John", lastName: "Doe"}}) -> "Testator: John Doe"
    - format_will_summary({beneficiaries: [{fullName: "Sarah Doe"}]}) -> includes "Beneficiaries: Sarah Doe"
    - format_will_summary({}) -> "No data collected yet."

    Extraction schemas:
    - ExtractedBeneficiary: full_name, relationship, id_number?, share_percent?, is_charity
    - ExtractedAsset: asset_type, description, details?
    - ExtractedGuardian: full_name, relationship, is_primary
    - ExtractedExecutor: name, relationship?, is_professional, backup_name?
    - ExtractedWillData: beneficiaries[], assets[], guardians[], executor?, bequests[], needs_clarification[]

    OpenAIService:
    - __init__(api_key, model="gpt-4o-mini") -- stores AsyncOpenAI client
    - stream_response(messages, section, will_context) -> AsyncGenerator[str, None]
    - extract_will_data(conversation_history, latest_message) -> ExtractedWillData
  </behavior>
  <implementation>
    1. Add `openai` to backend/requirements.txt

    2. Add OPENAI_API_KEY and OPENAI_MODEL to Settings in config.py

    3. Create backend/app/prompts/__init__.py (empty)

    4. Create backend/app/prompts/system.py with:
       - SECTION_PROMPTS dict mapping section names to specific guidance strings (from RESEARCH.md code example)
       - build_system_prompt(section, will_context) function that assembles base personality + section guidance + will state
       - format_will_summary(will_context) helper
       - Base personality per CONTEXT.md: friendly, approachable, not legalese, not overly formal
       - Tone calibration: "passing" not "dying", empathetic for children, neutral for family, reassuring for finances

    5. Create backend/app/prompts/extraction.py with:
       - ExtractedBeneficiary, ExtractedAsset, ExtractedGuardian, ExtractedExecutor Pydantic models
       - ExtractedWillData composite model
       - EXTRACTION_SYSTEM_PROMPT constant

    6. Create backend/app/services/openai_service.py with OpenAIService class:
       - Constructor takes api_key and model name
       - stream_response: creates async streaming completion with system prompt + messages, yields content chunks
       - extract_will_data: uses client.chat.completions.parse() with ExtractedWillData response_format (OpenAI Structured Outputs)
       - Use temperature=0.7 for conversation, temperature=0.2 for extraction
       - max_tokens=1024 for streaming, 512 for extraction

    7. Tests (test_openai_service.py):
       - Test build_system_prompt includes base personality for each section
       - Test build_system_prompt includes section-specific guidance
       - Test format_will_summary with various will states (empty, partial, full)
       - Test ExtractedWillData schema validates correctly
       - Test extraction schemas accept valid data and reject invalid
       - Do NOT test actual OpenAI API calls (mock the client for unit tests)
  </implementation>
</feature>

<verification>
1. Tests pass: `cd backend && python -m pytest tests/test_openai_service.py -v`
2. Service imports: `python -c "from app.services.openai_service import OpenAIService; print('OK')"`
3. Prompts import: `python -c "from app.prompts.system import build_system_prompt; from app.prompts.extraction import ExtractedWillData; print('OK')"`
</verification>

<success_criteria>
- OpenAIService class with streaming + extraction methods
- Section-specific prompts for: beneficiaries, assets, guardians, executor, bequests, residue
- All prompts include UPL boundary, friendly tone, will state context
- Extraction schemas match SA will data fields
- Tests pass for prompt building and schema validation
</success_criteria>

<output>
After completion, create `.planning/phases/03-core-will-conversation/03-02-SUMMARY.md`
</output>
