---
phase: 03-core-will-conversation
plan: 07
type: execute
wave: 3
depends_on: ["03-03", "03-05"]
files_modified:
  - frontend/src/features/will/hooks/useConversation.ts
  - frontend/src/features/will/components/ChatSection.tsx
  - frontend/src/features/will/components/ChatMessage.tsx
  - frontend/src/features/will/components/WillWizard.tsx
  - frontend/src/services/api.ts
autonomous: true

must_haves:
  truths:
    - "User can type a message and see AI response stream in real-time via SSE"
    - "Chat uses DaisyUI chat bubbles with distinct styling for user and AI messages"
    - "Chat auto-scrolls to latest message"
    - "AI messages show typing indicator while streaming"
    - "Chat section changes when user navigates to different will sections"
  artifacts:
    - path: "frontend/src/features/will/hooks/useConversation.ts"
      provides: "SSE streaming hook with sendMessage, messages, isStreaming state"
      contains: "useConversation"
    - path: "frontend/src/features/will/components/ChatSection.tsx"
      provides: "Full chat interface with message list, input, send button"
      contains: "ChatSection"
    - path: "frontend/src/features/will/components/ChatMessage.tsx"
      provides: "DaisyUI chat bubble wrapper"
      contains: "ChatMessage"
  key_links:
    - from: "frontend/src/features/will/hooks/useConversation.ts"
      to: "POST /api/conversation/stream"
      via: "fetch with SSE parsing"
    - from: "frontend/src/features/will/components/ChatSection.tsx"
      to: "frontend/src/features/will/hooks/useConversation.ts"
      via: "useConversation() hook"
    - from: "frontend/src/features/will/components/WillWizard.tsx"
      to: "frontend/src/features/will/components/ChatSection.tsx"
      via: "Renders ChatSection for AI-driven sections"
---

<objective>
Build the conversational AI chat interface with SSE streaming, DaisyUI chat bubbles, and integrate it into the wizard for all AI-driven sections (beneficiaries, assets, guardians, executor, bequests, residue).

Purpose: This is the primary interaction point where users describe their wishes in natural language and the AI guides them through each will section. SSE streaming provides real-time typing feedback for a natural conversation feel.
Output: useConversation hook, ChatMessage component, ChatSection interface, WillWizard integration.
</objective>

<execution_context>
@/home/laudes/.claude/get-shit-done/workflows/execute-plan.md
@/home/laudes/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/STATE.md
@.planning/phases/03-core-will-conversation/03-RESEARCH.md
@.planning/phases/03-core-will-conversation/03-CONTEXT.md
@.planning/phases/03-core-will-conversation/03-03-SUMMARY.md
@.planning/phases/03-core-will-conversation/03-05-SUMMARY.md
@frontend/src/features/will/components/WillWizard.tsx
@frontend/src/features/will/store/useWillStore.ts
@frontend/src/services/api.ts
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create useConversation SSE streaming hook and ChatMessage component</name>
  <files>
    frontend/src/features/will/hooks/useConversation.ts
    frontend/src/features/will/components/ChatMessage.tsx
    frontend/src/services/api.ts
  </files>
  <action>
    Create useConversation.ts (adapted from RESEARCH.md SSE Streaming Hook pattern):
    - Props: { section: string, willContext: Record<string, unknown>, willId: string | null }
    - State: messages (Message[]), isStreaming (boolean), error (string | null)
    - AbortController ref for cancellation
    - sendMessage(userMessage: string) async function:
      * Add user message to local messages state
      * Add empty assistant message placeholder
      * POST to /api/conversation/stream with body: { will_id: willId, messages: last N messages, current_section: section, will_context: willContext }
      * Include credentials: 'include' for POPIA cookie
      * Parse SSE response using ReadableStream reader:
        - Read chunks, decode with TextDecoder
        - Parse SSE event lines (lines starting with "event:" and "data:")
        - On "delta" event: append content to last assistant message
        - On "filtered" event: replace last assistant message with filtered content
        - On "done" event: finalize message
      * Handle errors gracefully (network, abort)
    - stopStreaming() function: abort current request
    - loadHistory(willId, section) function: GET /api/conversation/{willId}/{section} to restore prior messages
    - When section changes, load history for new section

    Create ChatMessage.tsx (from RESEARCH.md Pattern 5):
    - Props: { message: Message, isStreaming?: boolean }
    - DaisyUI chat component:
      * AI messages: chat chat-start, chat-bubble chat-bubble-neutral
      * User messages: chat chat-end, chat-bubble chat-bubble-primary
      * AI avatar: chat-image with "WC" placeholder (bg-primary text-primary-content)
    - When isStreaming and message is last assistant message, show DaisyUI loading dots (loading loading-dots) appended to content

    Add to frontend/src/services/api.ts:
    - getConversationHistory(willId: string, section: string) function
    - createWill(willType?: string) function
    - getWill(willId: string) function
    - updateWillSection(willId: string, section: string, data: unknown) function
    - markSectionComplete(willId: string, section: string) function
    These are typed wrappers around the will/conversation API endpoints.
  </action>
  <verify>cd frontend && npx tsc --noEmit 2>&1 | head -20</verify>
  <done>SSE streaming hook parses events, ChatMessage renders DaisyUI bubbles, API methods typed</done>
</task>

<task type="auto">
  <name>Task 2: Create ChatSection and integrate into WillWizard</name>
  <files>
    frontend/src/features/will/components/ChatSection.tsx
    frontend/src/features/will/components/WillWizard.tsx
  </files>
  <action>
    Create ChatSection.tsx:
    - The main conversational interface for AI-driven will sections
    - Props: { section: WillSection, willId: string }
    - Uses useConversation({ section, willContext: current will state from store, willId })
    - Layout (mobile-first, DaisyUI):
      * Top: Section-specific heading (e.g., "Let's talk about your beneficiaries")
      * Middle: Scrollable message list (flex-1, overflow-y-auto)
        - Map messages to ChatMessage components
        - Auto-scroll to bottom on new messages (useRef + scrollIntoView)
        - Empty state: AI greeting message for the section
      * Bottom: Sticky input area
        - textarea (DaisyUI textarea textarea-bordered) for message input
        - Send button (btn btn-primary btn-sm) -- disabled when streaming or empty input
        - Stop button (btn btn-ghost btn-sm) visible when streaming
    - On component mount: load conversation history for this section
    - AI greeting messages per section (initial prompt to user):
      * beneficiaries: "Let's talk about who you'd like to inherit from your estate. Who would you like to name as a beneficiary?"
      * assets: "Now let's go through your assets. What property, vehicles, bank accounts, or other assets would you like to include?"
      * guardians: "If you have children under 18, it's important to nominate a guardian. Would you like to discuss this?"
      * executor: "An executor manages your estate. Would you like to nominate someone, or would you prefer a professional executor?"
      * bequests: "Would you like to leave any specific items to specific people? This is optional."
      * residue: "Finally, how would you like the remainder of your estate distributed?"
    - Use dvh units for mobile viewport (h-[calc(100dvh-theme(spacing.32))] or similar)
    - Sticky input with position: sticky, bottom: 0

    Update WillWizard.tsx:
    - Replace placeholder divs for AI sections with ChatSection component
    - Pass current section and willId from store
    - If no willId exists yet, create a will when user completes personal details (call createWill API, store willId)
    - Ensure section switching loads appropriate component (PersonalForm/MaritalForm for personal, ChatSection for AI sections)
  </action>
  <verify>cd frontend && npx tsc --noEmit 2>&1 | head -20</verify>
  <done>ChatSection renders full chat interface with streaming, auto-scroll, mobile-friendly input; WillWizard integrates all sections</done>
</task>

</tasks>

<verification>
1. TypeScript compiles: cd frontend && npx tsc --noEmit
2. SSE hook exists: grep "useConversation" frontend/src/features/will/hooks/useConversation.ts
3. Chat bubbles used: grep "chat-bubble" frontend/src/features/will/components/ChatMessage.tsx
4. Streaming indicator: grep "loading" frontend/src/features/will/components/ChatMessage.tsx
5. WillWizard renders ChatSection: grep "ChatSection" frontend/src/features/will/components/WillWizard.tsx
6. API methods: grep "createWill\|getConversationHistory" frontend/src/services/api.ts
</verification>

<success_criteria>
- useConversation hook handles SSE streaming with delta/filtered/done events
- ChatMessage renders user/AI messages with DaisyUI chat bubbles
- ChatSection has scrollable message list + sticky input area
- AI greeting messages per section set conversational context
- Auto-scroll to latest message
- Loading dots during streaming
- WillWizard renders ChatSection for all AI-driven sections
- Mobile-friendly with dvh units and sticky input
</success_criteria>

<output>
After completion, create `.planning/phases/03-core-will-conversation/03-07-SUMMARY.md`
</output>
