---
phase: 03-core-will-conversation
plan: 05
type: execute
wave: 2
depends_on: ["03-01", "03-02"]
files_modified:
  - backend/app/services/conversation_service.py
  - backend/app/api/conversation.py
  - backend/app/main.py
  - backend/requirements.txt
autonomous: true

must_haves:
  truths:
    - "SSE streaming endpoint streams AI response chunks as server-sent events"
    - "UPL filter runs on the complete AI response before sending 'done' event"
    - "Conversation history persisted per will+section in database"
    - "Rolling message window (20 messages) prevents token limit issues"
  artifacts:
    - path: "backend/app/services/conversation_service.py"
      provides: "ConversationService managing message history, OpenAI calls, UPL filtering"
      contains: "class ConversationService"
    - path: "backend/app/api/conversation.py"
      provides: "SSE streaming endpoint POST /api/conversation/stream"
      contains: "EventSourceResponse"
  key_links:
    - from: "backend/app/api/conversation.py"
      to: "backend/app/services/conversation_service.py"
      via: "Depends() injection"
    - from: "backend/app/services/conversation_service.py"
      to: "backend/app/services/openai_service.py"
      via: "OpenAIService.stream_response()"
    - from: "backend/app/services/conversation_service.py"
      to: "backend/app/services/upl_filter.py"
      via: "UPLFilterService.filter_output()"
    - from: "backend/app/services/conversation_service.py"
      to: "backend/app/models/conversation.py"
      via: "Conversation model for history persistence"
---

<objective>
Build the conversation service and SSE streaming API endpoint that orchestrates AI responses with UPL filtering and conversation history persistence.

Purpose: This is the core integration point -- connecting the OpenAI service, UPL filter, and conversation history into a streaming endpoint that the frontend chat UI consumes. Implements the dual-event pattern (stream deltas + filter complete response) from RESEARCH.md.
Output: ConversationService class, SSE streaming endpoint, sse-starlette integration.
</objective>

<execution_context>
@/home/laudes/.claude/get-shit-done/workflows/execute-plan.md
@/home/laudes/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/STATE.md
@.planning/phases/03-core-will-conversation/03-RESEARCH.md
@.planning/phases/03-core-will-conversation/03-01-SUMMARY.md
@.planning/phases/03-core-will-conversation/03-02-SUMMARY.md
@backend/app/services/upl_filter.py
@backend/app/services/openai_service.py
@backend/app/main.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Install sse-starlette and create ConversationService</name>
  <files>
    backend/requirements.txt
    backend/app/services/conversation_service.py
  </files>
  <action>
    Add sse-starlette>=2.0.0 to backend/requirements.txt. Run pip install sse-starlette.

    Create backend/app/services/conversation_service.py:

    class ConversationService:
      - __init__(self, session: AsyncSession, openai_service: OpenAIService, upl_filter: UPLFilterService)

      - async get_or_create_conversation(self, will_id: UUID, section: str) -> Conversation
        * Query for existing conversation by will_id + section
        * If not found, create new one with empty messages list
        * Return Conversation model

      - async add_message(self, conversation: Conversation, role: str, content: str) -> None
        * Append {role, content, timestamp} to conversation.messages JSONB array
        * Save to DB

      - get_message_window(self, conversation: Conversation, window_size: int = 20) -> list[dict]
        * Return last `window_size` messages from conversation.messages
        * This implements the rolling window to prevent token limit issues (Pitfall 4 from RESEARCH.md)

      - async stream_ai_response(self, will_id: UUID, section: str, user_message: str, will_context: dict) -> AsyncGenerator[dict, None]
        * Get or create conversation for this will+section
        * Add user message to conversation history
        * Get message window (last 20 messages)
        * Call openai_service.stream_response() with messages, section, will_context
        * Accumulate full response while yielding delta events: {"event": "delta", "data": {"content": chunk}}
        * After streaming completes, run UPL filter on full response
        * If filter action is not ALLOW, yield filtered event: {"event": "filtered", "data": {"action": action, "content": filtered_text}}
        * Add assistant message to conversation history (use filtered text if filtered, original if allowed)
        * Yield done event: {"event": "done", "data": {"complete": true}}
        * IMPORTANT: This is the dual-event pattern from RESEARCH.md Pitfall 1 -- stream deltas for UX, then filter complete response

      - async extract_data_from_message(self, will_id: UUID, section: str, user_message: str) -> dict | None
        * Call openai_service.extract_will_data() on the latest user message
        * Return extracted data dict or None if nothing extracted
        * This runs SEPARATELY from streaming (not blocking the SSE flow)

    DI function: get_conversation_service() that assembles ConversationService with session, OpenAIService, UPLFilterService.
  </action>
  <verify>python -c "from app.services.conversation_service import ConversationService; print('OK')"</verify>
  <done>ConversationService with streaming, UPL filtering, history persistence, rolling window</done>
</task>

<task type="auto">
  <name>Task 2: Create SSE streaming conversation endpoint</name>
  <files>
    backend/app/api/conversation.py
    backend/app/main.py
  </files>
  <action>
    Create backend/app/api/conversation.py:

    router = APIRouter(prefix="/conversation", tags=["conversation"])

    Endpoints:
    - POST /api/conversation/stream
      * Request body: ConversationRequest (from schemas/conversation.py) -- must include will_id, messages (for context), current_section, will_context
      * Extract user_id from request.state.user
      * Verify the will belongs to the user (call will_service.get_will)
      * Return EventSourceResponse from sse-starlette:
        ```python
        from sse_starlette.sse import EventSourceResponse
        async def event_generator():
            async for event in conversation_service.stream_ai_response(
                will_id=body.will_id,
                section=body.current_section,
                user_message=body.messages[-1].content,  # latest user message
                will_context=body.will_context,
            ):
                yield {"event": event["event"], "data": json.dumps(event["data"])}
        return EventSourceResponse(event_generator())
        ```

    - GET /api/conversation/{will_id}/{section}
      * Return conversation history for a will+section
      * Returns ConversationResponse with all messages

    - POST /api/conversation/{will_id}/{section}/extract
      * Trigger data extraction from the latest conversation
      * Returns extracted will data (beneficiaries, assets, etc.)
      * This is called separately from streaming to avoid blocking the SSE flow

    Register in backend/app/main.py: import conversation router and add app.include_router(conversation.router)
  </action>
  <verify>cd backend && python -c "from app.api.conversation import router; print(f'Routes: {len(router.routes)}')"</verify>
  <done>SSE streaming endpoint operational, conversation history retrievable, extraction endpoint available</done>
</task>

</tasks>

<verification>
1. sse-starlette installed: pip show sse-starlette
2. Service imports: python -c "from app.services.conversation_service import ConversationService"
3. Router has routes: python -c "from app.api.conversation import router; print(len(router.routes))"
4. Main includes conversation router: grep "conversation.router" backend/app/main.py
5. EventSourceResponse used: grep "EventSourceResponse" backend/app/api/conversation.py
</verification>

<success_criteria>
- SSE streaming endpoint at POST /api/conversation/stream
- Dual-event pattern: delta events during streaming, filtered/done events after completion
- UPL filter applied to complete AI responses before final event
- Conversation history persisted per will+section
- Rolling 20-message window for API calls
- Extraction endpoint available for post-conversation data extraction
</success_criteria>

<output>
After completion, create `.planning/phases/03-core-will-conversation/03-05-SUMMARY.md`
</output>
